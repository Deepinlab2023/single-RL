{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZyTiV3I_cq6z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PPO"
      ],
      "metadata": {
        "id": "ZyTiV3I_cq6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym=='0.26.2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "UNMfxhgWWiyj",
        "outputId": "0216221a-316d-4483-8a8c-26a63a220327"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827617 sha256=7e416e059bd0135e9691f13ac5401f31595a6bbe780517ca4a9b7cdd17564029\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "8b8103efa0214945a1a8783032f95741"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "22cVCz3Pcl3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch as th\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch import nn\n",
        "import scipy.stats as st\n",
        "import itertools\n",
        "import random\n",
        "import gym\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hbHJDxqVByZ",
        "outputId": "79bbeec2-525f-4c95-dc81-3b81bb12f983"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "weJTGheOuwXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CartPoleEnv:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def pre_process(self, state, _):\n",
        "        return th.FloatTensor(state).unsqueeze(0)"
      ],
      "metadata": {
        "id": "ZG3sZ-daPtGx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor"
      ],
      "metadata": {
        "id": "X1IEdU_nc73T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, layers1_num, layers2_num, out_num):\n",
        "        super(Actor, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(layers1_num, layers2_num), nn.ReLU(),\n",
        "            nn.Linear(layers2_num, out_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, d_obs, deterministic=False):\n",
        "        logits = self.layers(d_obs)\n",
        "        if deterministic:\n",
        "            action = int(torch.argmax(logits[0]).detach().cpu().numpy())\n",
        "            action_prob = 1.0\n",
        "        else:\n",
        "            c = torch.distributions.Categorical(logits=logits)\n",
        "            action = int(c.sample().cpu().numpy()[0])\n",
        "            action_prob = float(c.probs[0, action].detach().cpu().numpy())\n",
        "        return action, action_prob\n",
        "\n",
        "    def convert_action(self, action, env_name):\n",
        "        if env_name == 'Pong-v0':\n",
        "            return action + 2\n",
        "        else:\n",
        "            return action  # No need to adjust for other environments\n",
        "\n",
        "    def ppo_loss(self, d_obs, action, action_prob, advantage, eps_clip):\n",
        "        vs = np.array([[1., 0.], [0., 1.]])  # TODO: Adjust according to your use case\n",
        "        ts = torch.FloatTensor(vs[action.cpu().numpy()])\n",
        "\n",
        "        logits = self.layers(d_obs)\n",
        "        r = torch.sum(F.softmax(logits, dim=1) * ts, dim=1) / action_prob\n",
        "        loss1 = r * advantage\n",
        "        loss2 = torch.clamp(r, 1 - eps_clip, 1 + eps_clip) * advantage\n",
        "        loss = -torch.min(loss1, loss2)\n",
        "        loss = torch.mean(loss)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "dJK4IIAfPtyk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Critic"
      ],
      "metadata": {
        "id": "Imhxn8BGjltj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, layers1_num, layers2_num):\n",
        "        super(Critic, self).__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(layers1_num, layers2_num),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(layers2_num, layers2_num),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(layers2_num, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        state_value = self.critic(state)\n",
        "        return state_value\n",
        "\n",
        "    def critic_loss(self, state_val, discounted_rewards):\n",
        "        loss = nn.MSELoss(reduction='mean')\n",
        "        loss = loss(state_val, discounted_rewards)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "UFWS0boJjo5y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Loop"
      ],
      "metadata": {
        "id": "F7bNU8mpdUcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOtrainer:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "\n",
        "    def train(self, env, actor, critic, nb_episodes, batch_size):\n",
        "        tester = Figure()  # Assuming Figure is defined elsewhere\n",
        "        params = ParametersPPO()\n",
        "        opt = th.optim.Adam([\n",
        "            {'params': actor.parameters(), 'lr': params.lr},\n",
        "            {'params': critic.parameters(), 'lr': params.lr_c}\n",
        "        ])\n",
        "        reward_sum_running_avg = None\n",
        "        reward_sum_running_avg_history = []\n",
        "        training_results = []\n",
        "        test_results = []\n",
        "\n",
        "        for it in range(nb_episodes):\n",
        "            d_obs_history, action_history, action_prob_history, reward_history = [], [], [], []\n",
        "            state_val_history = []\n",
        "            done_history = []\n",
        "            episode_rewards = 0\n",
        "\n",
        "            for ep in range(params.ep):\n",
        "                obs, prev_obs = env.reset(), None\n",
        "                obs = obs[0]\n",
        "                for t in range(params.t):\n",
        "                    d_obs = env.pre_process(obs, prev_obs)\n",
        "\n",
        "                    with th.no_grad():\n",
        "                        action, action_prob = actor(d_obs)\n",
        "\n",
        "                    state_val = critic(d_obs)\n",
        "                    prev_obs = obs\n",
        "                    obs, reward, done, truncated, _ = env.step(actor.convert_action(action, self.env_name))\n",
        "\n",
        "                    d_obs_history.append(d_obs)\n",
        "                    action_history.append(action)\n",
        "                    action_prob_history.append(action_prob)\n",
        "                    reward_history.append(reward)\n",
        "                    state_val_history.append(state_val)\n",
        "                    done_history.append(done)\n",
        "\n",
        "                    episode_rewards += reward\n",
        "\n",
        "                    if done:\n",
        "                        reward_sum = sum(reward_history[-t:])\n",
        "                        reward_sum_running_avg = 0.99 * reward_sum_running_avg + 0.01 * reward_sum if reward_sum_running_avg else reward_sum\n",
        "                        reward_sum_running_avg_history.append(reward_sum_running_avg)\n",
        "                        break\n",
        "\n",
        "            training_results.append(episode_rewards / params.ep)  # Average reward per episode\n",
        "\n",
        "            # Compute advantage\n",
        "            R = 0\n",
        "            discounted_rewards = []\n",
        "\n",
        "            for r, d in zip(reward_history[::-1], done_history[::-1]):\n",
        "                if self.env_name == 'Pong-v0' and r != 0:\n",
        "                    R = 0  # Scored/lost a point in pong, so reset reward sum\n",
        "                if d is True:\n",
        "                    R = 0  # If terminal, R=0\n",
        "                R = r + params.gamma * R\n",
        "                discounted_rewards.insert(0, R)\n",
        "\n",
        "            # Normalizing the rewards\n",
        "            discounted_rewards = th.FloatTensor(discounted_rewards)\n",
        "            discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / discounted_rewards.std()\n",
        "            assert len(discounted_rewards) == len(state_val_history)\n",
        "            advantage_history = []\n",
        "            for i_adv in range(len(discounted_rewards)):\n",
        "                adv = discounted_rewards[i_adv] - state_val_history[i_adv]\n",
        "                advantage_history.append(adv)\n",
        "            assert len(advantage_history) == len(discounted_rewards)\n",
        "\n",
        "            # Update policy\n",
        "            for _ in range(params.training_times):\n",
        "                idxs = random.sample(range(len(action_history)), batch_size)\n",
        "                d_obs_batch = th.cat([d_obs_history[idx] for idx in idxs], 0)\n",
        "                action_batch = th.LongTensor([action_history[idx] for idx in idxs])\n",
        "                action_prob_batch = th.FloatTensor([action_prob_history[idx] for idx in idxs])\n",
        "                advantage_batch = th.FloatTensor([advantage_history[idx] for idx in idxs])\n",
        "                state_val_batch = th.FloatTensor([state_val_history[idx] for idx in idxs])\n",
        "                discounted_rewards_batch = th.FloatTensor([discounted_rewards[idx] for idx in idxs])\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss_a = actor.ppo_loss(d_obs_batch, action_batch, action_prob_batch, advantage_batch, params.eps_clip)\n",
        "                loss_c = critic.critic_loss(state_val_batch, discounted_rewards_batch)\n",
        "                loss = loss_a + loss_c\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            if it % params.test_interval == 0:\n",
        "                # Test 10 times for more accurate results\n",
        "                test_sum = 0\n",
        "                for test_i in range(params.test_trials):\n",
        "                    test_reward = tester.test(env, actor, self.env_name)\n",
        "                    test_sum += test_reward\n",
        "                test_average = test_sum / params.test_trials\n",
        "                test_results.append(test_average)\n",
        "                print('Training reward for episode %d: %.2f' % (it, test_average))\n",
        "\n",
        "            if it % params.save_episode == 0:\n",
        "                if it == 0:\n",
        "                    th.save({'actor': actor.state_dict(), 'critic': critic.state_dict()}, 'params.ckpt')\n",
        "                else:\n",
        "                    if test_average >= max(test_results[:-1]):\n",
        "                        th.save({'actor': actor.state_dict(), 'critic': critic.state_dict()}, 'params.ckpt')\n",
        "\n",
        "        return training_results, test_results"
      ],
      "metadata": {
        "id": "kk75R1lDPuVl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Loop"
      ],
      "metadata": {
        "id": "uNPh1rTfdY5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Figure:\n",
        "    def test(self, env, agent, env_name):\n",
        "        obs, prev_obs = env.reset(), None\n",
        "        obs = obs[0]\n",
        "        reward_sum = 0\n",
        "        reward_history = []\n",
        "        params = ParametersPPO()\n",
        "        for t in range(params.t):\n",
        "            d_obs = env.pre_process(obs, prev_obs)\n",
        "\n",
        "            with th.no_grad():\n",
        "                action, action_prob = agent(d_obs)\n",
        "\n",
        "            prev_obs = obs\n",
        "            obs, reward, done, truncated, _ = env.step(agent.convert_action(action, env_name))\n",
        "\n",
        "            reward_sum += reward\n",
        "            reward_history.append(reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return reward_sum"
      ],
      "metadata": {
        "id": "i3DQDjb0PuzY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "Vd3qmb_MddnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Utils:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def benchmark_plot(self, all_train_returns, all_test_returns, test_interval, moving_avg_window=100, down_sample_factor=100):\n",
        "        num_trials = len(all_train_returns)\n",
        "        num_points = len(all_test_returns[0])\n",
        "\n",
        "        # Convert lists to numpy arrays for easier calculations\n",
        "        all_train_returns = np.array(all_train_returns)\n",
        "        all_test_returns = np.array(all_test_returns)\n",
        "\n",
        "        # Calculate the mean and 95% confidence intervals\n",
        "        mean_train_returns = all_train_returns.mean(axis=0)\n",
        "        mean_test_returns = all_test_returns.mean(axis=0)\n",
        "\n",
        "        train_ci = 1.96 * all_train_returns.std(axis=0) / np.sqrt(num_trials)\n",
        "        test_ci = 1.96 * all_test_returns.std(axis=0) / np.sqrt(num_trials)\n",
        "\n",
        "        # Calculate individual maximum returns from each trial\n",
        "        individual_max_returns = [np.max(trial_returns) for trial_returns in all_test_returns]\n",
        "\n",
        "        # Calculate the average maximum return\n",
        "        avg_max_return = np.mean(individual_max_returns)\n",
        "\n",
        "        # Calculate the 95% confidence interval for the average maximum return\n",
        "        n = len(individual_max_returns)\n",
        "        sample_std = np.std(individual_max_returns, ddof=1)\n",
        "        t_value = st.t.ppf(1 - 0.025, df=n - 1)\n",
        "        margin_of_error = t_value * sample_std / np.sqrt(n)\n",
        "        avg_max_return_ci = margin_of_error\n",
        "\n",
        "        # Apply moving average to smooth the training returns\n",
        "        smoothed_mean_train_returns = np.convolve(mean_train_returns, np.ones(moving_avg_window) / moving_avg_window, mode='valid')\n",
        "        smoothed_train_ci = np.convolve(train_ci, np.ones(moving_avg_window) / moving_avg_window, mode='valid')\n",
        "\n",
        "        # Down-sample the training returns for plotting\n",
        "        down_sampled_indices = np.arange(0, len(smoothed_mean_train_returns), down_sample_factor)\n",
        "        down_sampled_mean_train_returns = smoothed_mean_train_returns[down_sampled_indices]\n",
        "        down_sampled_train_ci = smoothed_train_ci[down_sampled_indices]\n",
        "\n",
        "        # Plot training returns with moving average and confidence interval\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(down_sampled_indices, down_sampled_mean_train_returns, label='Mean Training Returns (Smoothed)', color='blue')\n",
        "        plt.fill_between(down_sampled_indices, down_sampled_mean_train_returns - down_sampled_train_ci, down_sampled_mean_train_returns + down_sampled_train_ci, color='lightblue', alpha=0.3, label='CI')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Training Return')\n",
        "        plt.title('Training Returns with 95% Confidence Interval (Smoothed)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot density plot of training returns\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        #sns.kdeplot(mean_train_returns, fill=True, label='Density Plot')\n",
        "        sns.kdeplot(mean_train_returns, label='Density Plot')\n",
        "        plt.xlabel('Training Return')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Density Plot of Training Returns')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot test returns\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        episodes = np.arange(0, num_points * test_interval, test_interval)\n",
        "        for i in range(num_trials):\n",
        "            plt.plot(episodes, all_test_returns[i], linestyle='dotted', alpha=0.5, label=f'Trial {i+1}')  # Individual test trials\n",
        "        plt.plot(episodes, mean_test_returns, '-o', label='Mean Test Returns', color='black')  # Mean test returns without error bars\n",
        "        plt.fill_between(episodes, mean_test_returns - test_ci, mean_test_returns + test_ci, color='lightblue', alpha=0.3, label='CI')  # Fill between upper and lower bounds\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Test Return')\n",
        "        plt.title('Test Returns with 95% Confidence Interval')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot density plot of test returns\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        #sns.kdeplot(mean_test_returns, fill=True, label='Density Plot')\n",
        "        sns.kdeplot(mean_test_returns, label='Density Plot')\n",
        "        plt.xlabel('Test Return')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Density Plot of Test Returns')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return mean_test_returns, avg_max_return, avg_max_return_ci, individual_max_returns"
      ],
      "metadata": {
        "id": "iIxNXG71PvR9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "yKnfQcwGdjBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParametersPPO:\n",
        "    def __init__(self):\n",
        "        self.nb_episodes = 1000  # or 500\n",
        "        self.batch_size = 128  # or 64\n",
        "        self.gamma = 0.99\n",
        "        self.eps_clip = 0.2\n",
        "        self.layers1_num = 4  # CartPole state space dimension\n",
        "        self.layers2_num = 64  # or 128, hidden_layer\n",
        "        self.out_num = 2  # CartPole action space dimension\n",
        "        self.lr = 3e-4  # learning rate for actor network\n",
        "        self.lr_c = 0.001  # learning rate for critic network\n",
        "        self.ep = 10\n",
        "        self.t = 500  # or 1000 the max time steps\n",
        "        self.training_times = 10\n",
        "        self.save_episode = 50\n",
        "        self.test_episode = 25\n",
        "        self.test_trials = 10  # test 10 times and get the average result\n",
        "        self.test_interval = 10  # test every 10 episodes\n",
        "        self.num_trials = 5\n",
        "\n",
        "class PPOrunner():\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def run_experiment(self):\n",
        "        params = ParametersPPO()\n",
        "        nb_episodes = params.nb_episodes\n",
        "        batch_size = params.batch_size\n",
        "        num_trials = params.num_trials\n",
        "\n",
        "        # Load_save_result = params.Load_save_result\n",
        "        all_train_returns = []\n",
        "        all_test_returns = []\n",
        "\n",
        "        for trial in range(num_trials):\n",
        "            print(f\"Trial: {trial+1}\")\n",
        "            actor_ppo = Actor(params.layers1_num, params.layers2_num, params.out_num)\n",
        "            critic_ppo = Critic(params.layers1_num, params.layers2_num)\n",
        "            trainer_ppo = PPOtrainer(self.env.env_name)\n",
        "\n",
        "            train_rewards, test_rewards = trainer_ppo.train(self.env, actor_ppo, critic_ppo, nb_episodes, batch_size)\n",
        "            all_train_returns.append(train_rewards)\n",
        "            all_test_returns.append(test_rewards)\n",
        "\n",
        "        utils = Utils()\n",
        "        average_returns, max_return, max_return_ci, individual_returns = utils.benchmark_plot(all_train_returns, all_test_returns, params.test_interval)\n",
        "        print(f\"Average Return: {average_returns}\")\n",
        "        print(f\"Max Return: {max_return}\")\n",
        "        print(f\"Max Return 95% CI: {max_return_ci}\")\n",
        "        print(f\"Individual Returns: {individual_returns}\")\n",
        "        print(\"Completed experiment\")\n",
        "\n",
        "def main():\n",
        "    env = CartPoleEnv('CartPole-v0')\n",
        "    runner = PPOrunner(env)\n",
        "    runner.run_experiment()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AT40xTZKOef-",
        "outputId": "23c469ed-028f-480e-fe05-2bfb06ceab7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial: 1\n",
            "Training reward for episode 0: 26.30\n",
            "Training reward for episode 10: 30.00\n",
            "Training reward for episode 20: 47.60\n",
            "Training reward for episode 30: 41.20\n",
            "Training reward for episode 40: 61.80\n",
            "Training reward for episode 50: 99.60\n",
            "Training reward for episode 60: 90.30\n",
            "Training reward for episode 70: 103.00\n",
            "Training reward for episode 80: 83.10\n",
            "Training reward for episode 90: 163.30\n",
            "Training reward for episode 100: 204.50\n",
            "Training reward for episode 110: 221.40\n",
            "Training reward for episode 120: 198.40\n",
            "Training reward for episode 130: 276.90\n",
            "Training reward for episode 140: 224.40\n",
            "Training reward for episode 150: 270.60\n",
            "Training reward for episode 160: 292.20\n",
            "Training reward for episode 170: 342.70\n",
            "Training reward for episode 180: 392.10\n",
            "Training reward for episode 190: 257.10\n",
            "Training reward for episode 200: 297.60\n",
            "Training reward for episode 210: 385.80\n",
            "Training reward for episode 220: 403.30\n",
            "Training reward for episode 230: 455.90\n",
            "Training reward for episode 240: 451.40\n",
            "Training reward for episode 250: 396.10\n",
            "Training reward for episode 260: 500.00\n",
            "Training reward for episode 270: 452.70\n",
            "Training reward for episode 280: 446.60\n",
            "Training reward for episode 290: 462.60\n",
            "Training reward for episode 300: 467.50\n",
            "Training reward for episode 310: 487.40\n",
            "Training reward for episode 320: 429.40\n",
            "Training reward for episode 330: 498.50\n",
            "Training reward for episode 340: 454.70\n",
            "Training reward for episode 350: 500.00\n",
            "Training reward for episode 360: 416.50\n",
            "Training reward for episode 370: 488.90\n",
            "Training reward for episode 380: 482.00\n",
            "Training reward for episode 390: 500.00\n",
            "Training reward for episode 400: 500.00\n",
            "Training reward for episode 410: 456.70\n",
            "Training reward for episode 420: 439.40\n",
            "Training reward for episode 430: 472.80\n",
            "Training reward for episode 440: 479.40\n",
            "Training reward for episode 450: 487.10\n",
            "Training reward for episode 460: 490.60\n",
            "Training reward for episode 470: 500.00\n",
            "Training reward for episode 480: 500.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-185149f1a366>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-185149f1a366>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-185149f1a366>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtrainer_ppo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtrain_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_ppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_ppo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_ppo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mall_train_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mall_test_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7623a5a975cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, actor, critic, nb_episodes, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mstate_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-ca3302e67b53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, d_obs, deterministic)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}